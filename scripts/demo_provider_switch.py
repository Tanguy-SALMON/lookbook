#!/usr/bin/env python3
"""
Demo script showing how to switch between Ollama and OpenRouter LLM providers.

This demonstrates the flexibility of the new provider system - you can easily
switch between local (Ollama) and cloud (OpenRouter) models with environment variables.
"""

import asyncio
import os
import sys
from pathlib import Path

# Add project root to Python path
project_root = Path(__file__).parent.parent
sys.path.insert(0, str(project_root))

from lookbook_mpc.adapters.llm_providers import LLMProviderFactory
from lookbook_mpc.adapters.intent import LLMIntentParser


async def demo_provider(provider_name: str, provider_config: dict):
    """Demonstrate a specific provider configuration."""
    print(f"\n{'=' * 60}")
    print(f"üß† Testing {provider_name}")
    print(f"{'=' * 60}")

    try:
        # Create provider
        provider = LLMProviderFactory.create_provider(**provider_config)
        print(f"‚úÖ Created provider: {provider.get_provider_name()}")

        # Test intent parsing
        parser = LLMIntentParser(provider)

        test_cases = [
            "I want to do yoga",
            "Going to dinner tonight, need to look good",
            "Business meeting outfit for tomorrow",
        ]

        for test_input in test_cases:
            print(f"\nüìù Input: '{test_input}'")
            try:
                result = await parser.parse_intent(test_input)
                print(f"   üéØ Activity: {result.get('activity', 'None')}")
                print(f"   üé™ Occasion: {result.get('occasion', 'None')}")
                print(f"   üé® Objectives: {result.get('objectives', [])}")
                print(f"   üí¨ Response: {result.get('natural_response', '')[:80]}...")
            except Exception as e:
                print(f"   ‚ùå Error: {e}")

        return True

    except Exception as e:
        print(f"‚ùå Failed to create {provider_name}: {e}")
        return False


async def demo_environment_switching():
    """Demonstrate switching providers via environment variables."""
    print(f"\n{'=' * 60}")
    print(f"üîÑ Environment Variable Switching Demo")
    print(f"{'=' * 60}")

    # Save original environment
    original_provider = os.getenv("LLM_PROVIDER")
    original_model = os.getenv("LLM_MODEL")

    configurations = [
        {
            "name": "Local Ollama (Development)",
            "env": {
                "LLM_PROVIDER": "ollama",
                "LLM_MODEL": "qwen3:4b-instruct",
                "OLLAMA_HOST": "http://localhost:11434",
            },
        },
        {
            "name": "OpenRouter Free (Production)",
            "env": {
                "LLM_PROVIDER": "openrouter",
                "LLM_MODEL": "qwen/qwen-2.5-7b-instruct:free",
                "OPENROUTER_API_KEY": os.getenv("OPENROUTER_API_KEY", "not-set"),
            },
        },
    ]

    for config in configurations:
        print(f"\nüîß Switching to: {config['name']}")

        # Set environment variables
        for key, value in config["env"].items():
            os.environ[key] = value
            print(f"   {key}={value}")

        # Test the configuration
        try:
            provider = LLMProviderFactory.create_from_env()
            print(f"‚úÖ Active provider: {provider.get_provider_name()}")

            # Quick test
            parser = LLMIntentParser(provider)
            result = await parser.parse_intent("Hello, I need outfit advice")
            print(f"‚úÖ Test passed: {result.get('natural_response', '')[:60]}...")

        except Exception as e:
            print(f"‚ùå Configuration failed: {e}")

    # Restore original environment
    if original_provider:
        os.environ["LLM_PROVIDER"] = original_provider
    elif "LLM_PROVIDER" in os.environ:
        del os.environ["LLM_PROVIDER"]

    if original_model:
        os.environ["LLM_MODEL"] = original_model
    elif "LLM_MODEL" in os.environ:
        del os.environ["LLM_MODEL"]


def show_configuration_examples():
    """Show configuration examples for different scenarios."""
    print(f"\n{'=' * 60}")
    print(f"üìã Configuration Examples")
    print(f"{'=' * 60}")

    examples = [
        {
            "title": "Development Setup (Local Ollama)",
            "description": "Fast local development with Ollama",
            "commands": [
                "# Start Ollama service",
                "ollama serve",
                "",
                "# Pull required models",
                "ollama pull qwen3:4b-instruct",
                "",
                "# Set environment variables",
                'export LLM_PROVIDER="ollama"',
                'export LLM_MODEL="qwen3:4b-instruct"',
                'export OLLAMA_HOST="http://localhost:11434"',
                "",
                "# Start lookbook service",
                "python main.py",
            ],
        },
        {
            "title": "Production Setup (OpenRouter Free)",
            "description": "Cloud-based inference with free models",
            "commands": [
                "# Get API key from https://openrouter.ai/keys",
                "",
                "# Set environment variables",
                'export LLM_PROVIDER="openrouter"',
                'export OPENROUTER_API_KEY="sk-or-v1-..."',
                'export LLM_MODEL="qwen/qwen-2.5-7b-instruct:free"',
                "",
                "# Start lookbook service",
                "python main.py",
            ],
        },
        {
            "title": "Hybrid Setup (OpenRouter with Ollama fallback)",
            "description": "Use OpenRouter when available, fallback to Ollama",
            "commands": [
                "# Start Ollama as fallback",
                "ollama serve",
                "ollama pull qwen3:4b-instruct",
                "",
                "# Configure OpenRouter as primary",
                'export LLM_PROVIDER="openrouter"',
                'export OPENROUTER_API_KEY="sk-or-v1-..."',
                'export LLM_MODEL="qwen/qwen-2.5-7b-instruct:free"',
                "",
                "# Ollama fallback configuration",
                'export OLLAMA_HOST="http://localhost:11434"',
                'export OLLAMA_TEXT_MODEL="qwen3:4b-instruct"',
                "",
                "# Start lookbook service",
                "python main.py",
            ],
        },
    ]

    for example in examples:
        print(f"\nüèóÔ∏è  {example['title']}")
        print(f"   {example['description']}")
        print()
        for command in example["commands"]:
            print(f"   {command}")


async def main():
    """Main demo function."""
    print("üöÄ Lookbook-MPC Flexible LLM Provider Demo")
    print("==========================================")
    print("This demonstrates switching between Ollama and OpenRouter providers")

    # Check what's available
    has_ollama = True  # Assume available for demo
    has_openrouter = bool(
        os.getenv("OPENROUTER_API_KEY") or os.getenv("OPENROUTER_KEY")
    )

    print(f"\nüìä Provider Availability:")
    print(f"   Ollama: {'‚úÖ Available' if has_ollama else '‚ùå Not running'}")
    print(f"   OpenRouter: {'‚úÖ API key found' if has_openrouter else '‚ùå No API key'}")

    # Demo configurations
    configurations = []

    if has_ollama:
        configurations.append(
            {
                "name": "Ollama (Local)",
                "config": {
                    "provider_type": "ollama",
                    "model": "qwen3:4b-instruct",
                    "host": "http://localhost:11434",
                    "timeout": 30,
                },
            }
        )

    if has_openrouter:
        api_key = os.getenv("OPENROUTER_API_KEY") or os.getenv("OPENROUTER_KEY")
        configurations.append(
            {
                "name": "OpenRouter (Cloud)",
                "config": {
                    "provider_type": "openrouter",
                    "model": "qwen/qwen-2.5-7b-instruct:free",
                    "api_key": api_key,
                    "timeout": 60,
                },
            }
        )

    # Run demonstrations
    if configurations:
        for config in configurations:
            success = await demo_provider(config["name"], config["config"])
            if success:
                print(f"‚úÖ {config['name']} working correctly")
            else:
                print(f"‚ùå {config['name']} failed")

        # Demo environment switching
        await demo_environment_switching()
    else:
        print("\n‚ö†Ô∏è  No providers available for demonstration")
        print("   Set up Ollama or add OPENROUTER_API_KEY to test")

    # Show configuration examples
    show_configuration_examples()

    # Summary
    print(f"\n{'=' * 60}")
    print("‚úÖ Flexible LLM Demo Complete!")
    print(f"{'=' * 60}")
    print("\nüéØ Key Benefits:")
    print("‚Ä¢ Easy switching between local and cloud models")
    print("‚Ä¢ Environment-driven configuration")
    print("‚Ä¢ Automatic fallback from OpenRouter to Ollama")
    print("‚Ä¢ Consistent API across all providers")
    print("‚Ä¢ Cost optimization (free models available)")

    print("\nüîß Quick Start:")
    if not has_openrouter:
        print("‚Ä¢ Get free OpenRouter API key: https://openrouter.ai/keys")
    if not has_ollama:
        print("‚Ä¢ Install Ollama: https://ollama.ai/")
    print("‚Ä¢ Set LLM_PROVIDER=openrouter to use cloud models")
    print("‚Ä¢ Set LLM_PROVIDER=ollama to use local models")

    print("\nüìö See FLEXIBLE_LLM_SETUP.md for detailed configuration guide")


if __name__ == "__main__":
    asyncio.run(main())
