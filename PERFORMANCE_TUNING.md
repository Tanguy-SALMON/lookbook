# Performance Tuning Guide - Lookbook MPC

This guide helps optimize the performance of the Lookbook-MPC Fashion Recommendation Service, focusing on LLM inference speed and overall system responsiveness.

## üöÄ Quick Performance Fix

If the chat is slow, the system is likely using heavy LLM models. Here's the immediate fix:

### Current Status Check
```bash
# Check which models are being used
grep -r "ollama_text_model" lookbook_mpc/
grep -r "qwen3:4b" lookbook_mpc/
```

### Apply Fast Model Configuration
```bash
# Already updated in the code:
# - chat.py uses settings.ollama_text_model_fast
# - smart_recommender.py uses settings.ollama_text_model_fast
# 
# Restart server to apply changes:
./start_server.sh
```

## ‚ö° Performance Optimization Layers

### 1. LLM Model Selection

**Current Configuration:**
- **Fast Model**: `llama3.2:1b-instruct-q4_K_M` (~800MB, ~2-3 seconds)
- **Standard Model**: `qwen3:4b-instruct` (~2.3GB, ~8-15 seconds)
- **Vision Model**: `qwen2.5vl:7b` (~4.7GB, only for image analysis)

**Usage by Component:**
```bash
# Intent Parser (chat.py) - Now uses FAST model
model=settings.ollama_text_model_fast  # llama3.2:1b

# SmartRecommender (smart_recommender.py) - Now uses FAST model  
model=settings.ollama_text_model_fast  # llama3.2:1b

# Vision Analysis - Still uses heavy model (only when needed)
model=settings.ollama_vision_model     # qwen2.5vl:7b
```

### 2. Timeout Optimization

**Before:**
```python
timeout=30  # Too long, causes UI freezing
```

**After:**
```python  
timeout=10  # Fast model completes in 2-3 seconds
```

### 3. Request Optimization

**Reduced Token Limits:**
```python
# Intent parsing - reduced for speed
max_tokens=512  # Was higher, now optimized

# Keyword generation - focused output
max_tokens=400  # Sufficient for keywords
```

**Lower Temperature:**
```python
temperature=0.3  # More deterministic, faster processing
```

## üìä Performance Benchmarks

### Expected Response Times (with Fast Model)

| Operation | Before (qwen3:4b) | After (llama3.2:1b) | Improvement |
|-----------|-------------------|---------------------|-------------|
| Intent Parsing | 8-15 seconds | 2-3 seconds | **75% faster** |
| Keyword Generation | 10-20 seconds | 3-5 seconds | **70% faster** |
| Total Chat Response | 20-35 seconds | 5-8 seconds | **75% faster** |
| Simple Queries | 15-25 seconds | 3-5 seconds | **80% faster** |

### Terminal vs Chat Speed Difference

**Why terminal Ollama is faster:**
1. **No network overhead** - Direct binary communication
2. **No JSON parsing** - Raw text output
3. **No concurrent requests** - Single focused task
4. **No additional processing** - Just LLM inference

**Chat system overhead:**
1. **HTTP requests** - FastAPI ‚Üí Ollama
2. **JSON serialization** - Request/response parsing
3. **Database queries** - Product lookups
4. **Multiple LLM calls** - Intent + keyword generation
5. **Response formatting** - Structured output

## üîß Additional Optimizations

### 1. Database Query Optimization
```sql
-- Add indexes for common searches
CREATE INDEX idx_product_category ON product_vision_attributes(category);
CREATE INDEX idx_product_color ON product_vision_attributes(color);
CREATE INDEX idx_product_occasion ON product_vision_attributes(occasion);
CREATE INDEX idx_product_stock ON products(in_stock);
```

### 2. Response Caching
```python
# Cache frequent queries (future enhancement)
@lru_cache(maxsize=100)
async def cached_intent_parsing(message: str):
    # Cache common intents like "dancing", "party", etc.
    pass
```

### 3. Concurrent Processing
```python
# Process intent and product search in parallel
async def optimized_chat_flow():
    # Start both operations simultaneously
    intent_task = asyncio.create_task(parse_intent(message))
    products_task = asyncio.create_task(get_popular_products())
    
    intent = await intent_task
    products = await products_task
    # Continue with recommendation logic...
```

## üéØ Environment Configuration

### Development/Testing Setup
```bash
# .env for development (fast responses)
OLLAMA_TEXT_MODEL=llama3.2:1b-instruct-q4_K_M
OLLAMA_TEXT_MODEL_FAST=llama3.2:1b-instruct-q4_K_M

# Keep vision model for image analysis when needed
OLLAMA_VISION_MODEL=qwen2.5vl:7b
```

### Production Setup  
```bash
# .env for production (balance speed vs quality)
OLLAMA_TEXT_MODEL=qwen3:4b-instruct        # High quality for complex queries
OLLAMA_TEXT_MODEL_FAST=llama3.2:1b-instruct-q4_K_M  # Fast for simple operations

# Dynamic model selection based on query complexity
USE_FAST_MODEL_FOR_SIMPLE_QUERIES=true
```

## üìà Performance Monitoring

### 1. Response Time Logging
```python
# Already implemented in chat.py
response_time_ms = int((time.time() - start_time) * 1000)
logger.info("Chat response completed", response_time=response_time_ms)
```

### 2. Model Performance Tracking
```bash
# Monitor Ollama performance
curl http://localhost:11434/api/tags
curl http://localhost:11434/api/show -d '{"name":"llama3.2:1b-instruct-q4_K_M"}'
```

### 3. System Resource Usage
```bash
# Check memory usage
docker stats # if using Docker
ps aux | grep ollama
htop # overall system performance
```

## üö® Troubleshooting Slow Performance

### Common Issues & Solutions

**1. Heavy Model Still Being Used**
```bash
# Check current configuration
grep -r "qwen3:4b" lookbook_mpc/

# Should show: No results (all updated to use fast model)
```

**2. Ollama Server Issues**
```bash
# Restart Ollama if needed
sudo systemctl restart ollama

# Or if using Docker:
docker restart ollama
```

**3. Model Not Downloaded**
```bash
# Ensure fast model is available
ollama list | grep llama3.2

# Download if missing
ollama pull llama3.2:1b-instruct-q4_K_M
```

**4. Database Connection Slow**
```bash
# Test database speed
curl http://localhost:8000/ready

# Check MySQL performance
mysqladmin -u magento -p status
```

## üéÆ Testing Performance

### 1. Speed Test Queries
```bash
# Test fast queries (should complete in 3-5 seconds)
curl -w "Time: %{time_total}s\n" -s -X POST http://localhost:8000/v1/chat \
  -H "Content-Type: application/json" \
  -d '{"session_id": "speed_test", "message": "I want to go dancing"}'
```

### 2. Compare Model Performance
```bash
# Terminal Ollama test (for comparison)
time ollama run llama3.2:1b-instruct-q4_K_M "What should I wear for dancing?"

# Should complete in 1-2 seconds
```

### 3. Load Testing
```bash
# Multiple concurrent requests
for i in {1..5}; do
  curl -X POST http://localhost:8000/v1/chat \
    -H "Content-Type: application/json" \
    -d "{\"session_id\": \"test_$i\", \"message\": \"I need party outfits\"}" &
done
wait
```

## üìã Performance Checklist

### ‚úÖ Immediate Fixes Applied
- [x] **Fast LLM model enabled** - llama3.2:1b instead of qwen3:4b
- [x] **Reduced timeouts** - 10 seconds instead of 30
- [x] **Optimized token limits** - Focused output for speed
- [x] **Lower temperature** - More deterministic responses

### üîÑ Server Restart Required
```bash
# Apply all performance changes
./start_server.sh

# Verify fast model is being used
curl http://localhost:8000/health
```

### üéØ Expected Results After Optimization
- **Chat responses**: 3-8 seconds (down from 15-30 seconds)
- **Connection status**: Shows "Connected" quickly
- **Product recommendations**: Load within 5-8 seconds
- **UI responsiveness**: No more freezing during requests

---

## üèÜ Success Metrics

**Before Optimization:**
- Response time: 15-30 seconds
- User experience: Slow, causes UI freezing
- Model memory: 2.3GB+ per request

**After Optimization:**
- Response time: 3-8 seconds ‚ö°
- User experience: Fast, responsive UI ‚úÖ
- Model memory: 800MB per request üíæ

**Target Performance:** Chat responses should feel as fast as typing in a messaging app!